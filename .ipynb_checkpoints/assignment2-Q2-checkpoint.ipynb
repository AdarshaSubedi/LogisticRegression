{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf74bbe-011f-4085-896a-0055acc6ba2c",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a77a776-4368-4e1a-ba08-ea8d79202a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True) #disables the automatic downcasting of data types ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e102357-3d8d-409c-950c-3e63bee7daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Cancer_dataset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8689ac76-3f01-4289-9204-2814d1c83583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(data, cols):\n",
    "    filtered_data = data[columns]\n",
    "    for column in filtered_data.columns:\n",
    "        print(f\"################ {column} Stats ##################\")\n",
    "\n",
    "        col = df[column]\n",
    "        \n",
    "        print(f\"{column} count = {col.shape[0]}\")\n",
    "        print(f\"{column} mean = {col.mean()}\")\n",
    "        print(f\"{column} standard deviation = {col.std()}\")\n",
    "        print(f\"{column} minimum = {col.min()}\")\n",
    "        print(f\"{column} 25% percentile = {col.quantile(0.25)}\")\n",
    "        print(f\"{column} 50% percentile = {col.quantile(0.5)}\")\n",
    "        print(f\"{column} 75% percentile = {col.quantile(0.75)}\")\n",
    "        print(f\"{column} maximum = {col.max()}\")\n",
    "\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abba2e8e-601f-436a-9e8d-4094a70b5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', 'mean_smoothness', 'mean_compactness', 'mean_concavity','mean_concave_points']\n",
    "# stats(df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ba9e8a-12fd-441f-984f-f0fd307363a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows = 198\n",
      "unique values = ['N' 'R']\n",
      "top value = N\n",
      "top value frequency = 151\n",
      "unique value frequency = outcome\n",
      "N    151\n",
      "R     47\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Summarize the statistics of variable into count, unique value, top value, and frequency of top value. \n",
    "outcome = df['outcome']\n",
    "count = outcome.shape[0]\n",
    "print(f\"total rows = {count}\")\n",
    "\n",
    "unique_value = outcome.unique()\n",
    "print(f\"unique values = {unique_value}\")\n",
    "\n",
    "top_value = outcome.value_counts().index[0]\n",
    "print(f\"top value = {top_value}\")\n",
    "\n",
    "frequency_top = outcome.value_counts().max()\n",
    "print(f\"top value frequency = {frequency_top}\")  \n",
    "\n",
    "unique_val_counts = outcome.value_counts()\n",
    "print(f\"unique value frequency = {unique_val_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45804106-0ba2-4eec-a964-ff2c1768dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical to numerical\n",
    "# R: Recurrence of the cancer.\n",
    "# N: No recurrence of the cancer.\n",
    "\n",
    "#label encoding: replacing category to integer value (since the category is of only two type, One-hot Encoding is not used)\n",
    "encoded_outcome = outcome.replace({'R': 1, 'N': 0})\n",
    "# encoded_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d09cab1-2b07-4990-af3e-91e914b43ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation between mean_perimeter and se_perimeter = 0.6099643781634989\n"
     ]
    }
   ],
   "source": [
    "# correlation between mean_perimeter and se_perimeter\n",
    "print(f\"correlation between mean_perimeter and se_perimeter = {df['mean_perimeter'].corr(df['se_perimeter'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4f342-aa6d-4b61-9d95-abb69bc1f41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04be888-1ec0-4f01-a9b8-0bd4a4aa81ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83f437c2-dad6-4820-8242-ab0eccc3f23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>outcome</th>\n",
       "      <th>time</th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "      <th>tumor_size</th>\n",
       "      <th>lymph_node_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119513</td>\n",
       "      <td>N</td>\n",
       "      <td>31</td>\n",
       "      <td>18.02</td>\n",
       "      <td>27.60</td>\n",
       "      <td>117.50</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.09489</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.1086</td>\n",
       "      <td>...</td>\n",
       "      <td>139.70</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.1195</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.08113</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8423</td>\n",
       "      <td>N</td>\n",
       "      <td>61</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>...</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>842517</td>\n",
       "      <td>N</td>\n",
       "      <td>116</td>\n",
       "      <td>21.37</td>\n",
       "      <td>17.44</td>\n",
       "      <td>137.50</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>0.08836</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>...</td>\n",
       "      <td>159.10</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.3449</td>\n",
       "      <td>0.3414</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.09067</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>843483</td>\n",
       "      <td>N</td>\n",
       "      <td>123</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.2839</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>...</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>843584</td>\n",
       "      <td>R</td>\n",
       "      <td>27</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.1328</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>...</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id outcome  time  mean_radius  mean_texture  mean_perimeter  mean_area  \\\n",
       "0  119513       N    31        18.02         27.60          117.50     1013.0   \n",
       "1    8423       N    61        17.99         10.38          122.80     1001.0   \n",
       "2  842517       N   116        21.37         17.44          137.50     1373.0   \n",
       "3  843483       N   123        11.42         20.38           77.58      386.1   \n",
       "4  843584       R    27        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   mean_smoothness  mean_compactness  mean_concavity  ...  worst_perimeter  \\\n",
       "0          0.09489            0.1036          0.1086  ...           139.70   \n",
       "1          0.11840            0.2776          0.3001  ...           184.60   \n",
       "2          0.08836            0.1189          0.1255  ...           159.10   \n",
       "3          0.14250            0.2839          0.2414  ...            98.87   \n",
       "4          0.10030            0.1328          0.1980  ...           152.20   \n",
       "\n",
       "   worst_area  worst_smoothness  worst_compactness  worst_concavity  \\\n",
       "0      1436.0            0.1195             0.1926           0.3140   \n",
       "1      2019.0            0.1622             0.6656           0.7119   \n",
       "2      1949.0            0.1188             0.3449           0.3414   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst_concave_points  worst_symmetry  worst_fractal_dimension  tumor_size  \\\n",
       "0                0.1170          0.2677                  0.08113         5.0   \n",
       "1                0.2654          0.4601                  0.11890         3.0   \n",
       "2                0.2032          0.4334                  0.09067         2.5   \n",
       "3                0.2575          0.6638                  0.17300         2.0   \n",
       "4                0.1625          0.2364                  0.07678         3.5   \n",
       "\n",
       "   lymph_node_status  \n",
       "0                5.0  \n",
       "1                2.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e078ef6-5dad-4689-b014-a17e78450ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6d66afb-111d-47ea-8fca-b74f103a4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eebdb530-a03b-45af-892b-47788576a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0710739-1cd7-4d8e-900d-5b3fa0dcbeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = df.drop(columns=['outcome', 'id']).corr()\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "# plt.title(\"Correlation Matrix Heatmap\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9688f5-6681-4672-a5c8-0aa0acfb4b36",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565a134d-3138-4b6c-8b41-2efe5e47b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True) #disables the automatic downcasting of data types ()\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf2a36c-add6-4fce-89cd-1372106f51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class manipulates data from a csv file\n",
    "class ManipulateData:\n",
    "\n",
    "    #initialize file\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "\n",
    "    #Reads the supplied file and shuffles the data\n",
    "    def filter_data(self):\n",
    "        data = pd.read_csv(self.file).dropna()\n",
    "        data = data.sample(frac = 1, random_state=30) #shuffling and ensuring everytime output is same shuffled data\n",
    "        return data\n",
    "\n",
    "    #Filter with respect to column passed as an argument \n",
    "    def seperate_columns(self, data, columns):\n",
    "        data = data[columns]\n",
    "        return data\n",
    "\n",
    "    #splits data to train and test data and returns train and test data as numpy array\n",
    "    #only training percentage is passed as an argument \n",
    "    def split_data(self, data, percent):\n",
    "        data_array = data.to_numpy()\n",
    "        train_size = math.floor(len(data_array)*percent) #takes integer as training size\n",
    "        #training and testing data are seperated as two numpy arrays inside tuple\n",
    "        train_data, test_data = data_array[:train_size,:], data_array[train_size:,:] \n",
    "\n",
    "        \n",
    "        train_length = train_data.shape[0]\n",
    "        test_length = test_data.shape[0]\n",
    "        print(f\"total training rows: {train_length}\\ntotal testing rows: {test_length}\")\n",
    "        \n",
    "        return train_data, test_data\n",
    "\n",
    "    #seperate features and target\n",
    "    def seperate_target(self, data):\n",
    "        rows = data.shape[0]\n",
    "        features = []\n",
    "        target = []\n",
    "        for row in range(rows):\n",
    "            features.append(data[row, :-1])\n",
    "            target.append(data[row, -1])\n",
    "        return np.array(features), np.array(target)\n",
    "\n",
    "    # scale features using mean normalization and returns numpy array\n",
    "    def scale_features(self, data):\n",
    "        # calculate mean, minimum, and maximum for each column of the features\n",
    "        feature_mean = np.mean(data, axis=0) #axis 0 refers to the column and axis 1 for row\n",
    "        feature_max = np.max(data, axis=0)\n",
    "        feature_min = np.min(data, axis=0)\n",
    "\n",
    "        scaled_data = []\n",
    "\n",
    "        # normalize and append each rows of features to new array \n",
    "        for row in data:\n",
    "            scaled_row = []\n",
    "            for i in range(len(row)):\n",
    "                scaled_value = (row[i] - feature_mean[i]) / (feature_max[i] - feature_min[i])\n",
    "                scaled_row.append(scaled_value)\n",
    "            scaled_data.append(scaled_row)\n",
    "\n",
    "        return np.array(scaled_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f53e1dfb-d9f7-4fe7-b2e2-16e6298027b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and clear out null values\n",
    "file = 'Cancer_dataset1.csv'\n",
    "\n",
    "df = ManipulateData(file)\n",
    "data = df.filter_data() #cleans out rows containing null values and shuffles the data\n",
    "data['outcome'] = data['outcome'].replace({'R': 1, 'N': 0}).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86656ce-80a0-44d1-97f9-7064ce7eebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training rows: 145\n",
      "total testing rows: 37\n"
     ]
    }
   ],
   "source": [
    "#first seperate columns by passing column names and after required columns are obtained from the data we split 80% of data as training data\n",
    "#index of the columns defined will be the same index on the array as well\n",
    "columns = ['mean_area', 'outcome'] #the final column will be the target value when you use 'seperate_target(data) method\n",
    "sprtd_data = df.seperate_columns(data, columns)\n",
    "\n",
    "split_percent = 0.8\n",
    "train_data, test_data = df.split_data(sprtd_data, split_percent)\n",
    "\n",
    "# now seperate features and target as two seperate arrays with each features index corresponding to target index\n",
    "features, target = df.seperate_target(train_data)\n",
    "test_features, test_target = df.seperate_target(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977774ff-405d-4157-9f6e-d2b626b98d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LL:\n",
    "    # calculating cost using vectorization('np.dot()')\n",
    "    def calculate_cost(self, X_train, Y_train, th0, thj):\n",
    "            \n",
    "        m = X_train.shape[0]\n",
    "\n",
    "        # use vectorization to find dot product of two matrices (features and weights)\n",
    "        y_predict = th0 + np.dot(X_train, thj)\n",
    "\n",
    "        sigmoid = 1/(1+np.exp(-y_predict))\n",
    "        \n",
    "        cost = (Y_train * np.log(sigmoid) + ((1-Y_train) * np.log(1 - sigmoid)))\n",
    "        \n",
    "        total_cost = np.sum(cost) * (-1/m)\n",
    "    \n",
    "        return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0df80476-3e00-45bc-908f-bd7df0b51b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LL):\n",
    "    def __init__(self, alpha, iterations):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "\n",
    "    #this method computes gradient using vectorization and utilizes the broadcasting power of numpy\n",
    "    def calculate_gradient(self, X_train, Y_train, th0, thj):\n",
    "        m = X_train.shape[0]\n",
    "        \n",
    "        y_predict = th0 + np.dot(X_train, thj) #get numpy array containing predicted y of each row\n",
    "        print(y_predict)\n",
    "\n",
    "        e = 1 / (1 + np.exp(-y_predict)) # (broadcasting is used to do matrix manipulation)\n",
    "        \n",
    "        gradient_th0 = np.sum(e) / m #difference is an array of differences so we need to sum all and divide my number of rows\n",
    "\n",
    "        #vectorization used to find dot product and gets matrix with the same shape of weight\n",
    "        #broadcasting is utilized on difference and m\n",
    "        gradient_thj = np.dot(e, X_train) / m \n",
    "\n",
    "        return gradient_th0, gradient_thj\n",
    "\n",
    "    # utilizes gradient descent formula  to get optimal weights and bias\n",
    "    def gradient_descent(self, X_train, Y_train, th0, thj):\n",
    "\n",
    "        j_history = []\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            gradients = self.calculate_gradient(X_train, Y_train, th0, thj)\n",
    "            th0 = th0 - (self.alpha * gradients[0])\n",
    "            thj = thj - (self.alpha * gradients[1])\n",
    "\n",
    "        # check cost over iteration\n",
    "        if i%10 == 0 or i == self.iterations - 1:\n",
    "            current_cost = self.calculate_cost(X_train, Y_train, th0, thj)\n",
    "            j_history.append([i, current_cost])\n",
    "            # print(f\"Iteration: {i}\\t Cost: {current_cost:.3f}\\t Bias: {th0:.3f}\\t Weight: {np.round(thj, 3)}\")\n",
    "        \n",
    "        return th0, thj, j_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b05d524-4f24-4cfc-bcef-3d0aabfa7ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "[-0.00025 -0.00045 -0.00065 -0.00085 -0.00105 -0.00125 -0.00145]\n",
      "[-0.00049987 -0.00089977 -0.00129966 -0.00169956 -0.00209945 -0.00249935\n",
      " -0.00289924]\n",
      "[-0.00074962 -0.00134931 -0.00194899 -0.00254868 -0.00314836 -0.00374805\n",
      " -0.00434773]\n",
      "[-0.00099924 -0.00179861 -0.00259798 -0.00339735 -0.00419672 -0.00499609\n",
      " -0.00579546]\n",
      "[-0.00124874 -0.00224769 -0.00324664 -0.00424559 -0.00524454 -0.00624349\n",
      " -0.00724244]\n",
      "[-0.00149811 -0.00269653 -0.00389496 -0.00509339 -0.00629181 -0.00749024\n",
      " -0.00868866]\n",
      "[-0.00174735 -0.00314515 -0.00454294 -0.00594074 -0.00733854 -0.00873634\n",
      " -0.01013413]\n",
      "[-0.00199647 -0.00359353 -0.00519059 -0.00678766 -0.00838472 -0.00998178\n",
      " -0.01157885]\n",
      "[-0.00224546 -0.00404169 -0.00583791 -0.00763413 -0.00943036 -0.01122658\n",
      " -0.01302281]\n",
      "[-0.00249433 -0.00448961 -0.00648489 -0.00848017 -0.01047545 -0.01247073\n",
      " -0.01446602]\n",
      "[-0.00274307 -0.0049373  -0.00713154 -0.00932577 -0.01152    -0.01371424\n",
      " -0.01590847]\n",
      "[-0.00299168 -0.00538476 -0.00777785 -0.01017093 -0.01256401 -0.01495709\n",
      " -0.01735017]\n",
      "[-0.00324017 -0.005832   -0.00842382 -0.01101565 -0.01360747 -0.0161993\n",
      " -0.01879113]\n",
      "[-0.00348854 -0.006279   -0.00906947 -0.01185993 -0.0146504  -0.01744086\n",
      " -0.02023132]\n",
      "[-0.00373677 -0.00672577 -0.00971477 -0.01270377 -0.01569277 -0.01868177\n",
      " -0.02167077]\n",
      "[-0.00398489 -0.00717232 -0.01035975 -0.01354718 -0.01673461 -0.01992204\n",
      " -0.02310947]\n",
      "[-0.00423287 -0.00761863 -0.01100439 -0.01439015 -0.0177759  -0.02116166\n",
      " -0.02454742]\n",
      "[-0.00448074 -0.00806472 -0.0116487  -0.01523268 -0.01881666 -0.02240064\n",
      " -0.02598462]\n",
      "[-0.00472848 -0.00851057 -0.01229267 -0.01607477 -0.01985687 -0.02363897\n",
      " -0.02742106]\n",
      "[-0.00497609 -0.0089562  -0.01293631 -0.01691643 -0.02089654 -0.02487665\n",
      " -0.02885676]\n",
      "[-0.00522358 -0.0094016  -0.01357962 -0.01775764 -0.02193567 -0.02611369\n",
      " -0.03029171]\n",
      "[-0.00547094 -0.00984677 -0.0142226  -0.01859843 -0.02297426 -0.02735009\n",
      " -0.03172592]\n",
      "[-0.00571818 -0.01029171 -0.01486524 -0.01943877 -0.0240123  -0.02858584\n",
      " -0.03315937]\n",
      "[-0.00596529 -0.01073642 -0.01550755 -0.02027868 -0.02504981 -0.02982094\n",
      " -0.03459208]\n",
      "[-0.00621228 -0.0111809  -0.01614953 -0.02111816 -0.02608678 -0.03105541\n",
      " -0.03602404]\n",
      "[-0.00645914 -0.01162516 -0.01679118 -0.02195719 -0.02712321 -0.03228923\n",
      " -0.03745525]\n",
      "[-0.00670588 -0.01206919 -0.01743249 -0.0227958  -0.0281591  -0.03352241\n",
      " -0.03888571]\n",
      "[-0.00695249 -0.01251298 -0.01807347 -0.02363396 -0.02919445 -0.03475494\n",
      " -0.04031543]\n",
      "[-0.00719898 -0.01295655 -0.01871413 -0.0244717  -0.03022927 -0.03598684\n",
      " -0.04174441]\n",
      "[-0.00744535 -0.0133999  -0.01935445 -0.02530899 -0.03126354 -0.03721809\n",
      " -0.04317264]\n",
      "[-0.00769159 -0.01384301 -0.01999444 -0.02614586 -0.03229728 -0.0384487\n",
      " -0.04460012]\n",
      "[-0.00793771 -0.0142859  -0.02063409 -0.02698229 -0.03333048 -0.03967867\n",
      " -0.04602687]\n",
      "[-0.0081837  -0.01472856 -0.02127342 -0.02781828 -0.03436314 -0.040908\n",
      " -0.04745286]\n",
      "[-0.00842957 -0.01517099 -0.02191242 -0.02865384 -0.03539527 -0.04213669\n",
      " -0.04887812]\n",
      "[-0.00867531 -0.0156132  -0.02255109 -0.02948897 -0.03642686 -0.04336474\n",
      " -0.05030263]\n",
      "[-0.00892093 -0.01605518 -0.02318942 -0.03032366 -0.03745791 -0.04459215\n",
      " -0.05172639]\n",
      "[-0.00916643 -0.01649693 -0.02382743 -0.03115793 -0.03848842 -0.04581892\n",
      " -0.05314942]\n",
      "[-0.0094118  -0.01693845 -0.0244651  -0.03199175 -0.0395184  -0.04704505\n",
      " -0.0545717 ]\n",
      "[-0.00965705 -0.01737975 -0.02510245 -0.03282515 -0.04054785 -0.04827055\n",
      " -0.05599325]\n",
      "[-0.00990218 -0.01782082 -0.02573947 -0.03365811 -0.04157676 -0.0494954\n",
      " -0.05741405]\n",
      "[-0.01014718 -0.01826167 -0.02637616 -0.03449064 -0.04260513 -0.05071962\n",
      " -0.05883411]\n",
      "[-0.01039206 -0.01870229 -0.02701252 -0.03532274 -0.04363297 -0.0519432\n",
      " -0.06025343]\n",
      "[-0.01063681 -0.01914268 -0.02764855 -0.03615441 -0.04466028 -0.05316614\n",
      " -0.06167201]\n",
      "[-0.01088145 -0.01958285 -0.02828425 -0.03698565 -0.04568705 -0.05438845\n",
      " -0.06308985]\n",
      "[-0.01112595 -0.02002279 -0.02891962 -0.03781645 -0.04671328 -0.05561012\n",
      " -0.06450695]\n",
      "[-0.01137034 -0.0204625  -0.02955466 -0.03864683 -0.04773899 -0.05683115\n",
      " -0.06592331]\n",
      "[-0.0116146  -0.02090199 -0.03018938 -0.03947677 -0.04876416 -0.05805155\n",
      " -0.06733894]\n",
      "[-0.01185874 -0.02134125 -0.03082377 -0.04030628 -0.0497888  -0.05927131\n",
      " -0.06875383]\n",
      "[-0.01210275 -0.02178029 -0.03145783 -0.04113536 -0.0508129  -0.06049044\n",
      " -0.07016798]\n",
      "Iteration: 49\t Cost: 0.705\t Bias: -0.002\t Weight: [-0.01]\n",
      "\n",
      "\n",
      "final bias = -0.0024741867292928754 \t final weights = [-0.00987246]\n"
     ]
    }
   ],
   "source": [
    "#initialize weight, biases and hyperparameters\n",
    "bias = 0\n",
    "weights = np.zeros(features.shape[1])\n",
    "learning_rate = 0.0001\n",
    "iterations = 50\n",
    "\n",
    "#create object for linear regression class\n",
    "fit = LogisticRegression(learning_rate, iterations)\n",
    "\n",
    "\n",
    "\n",
    "# compute final weights and biases using gradient descent\n",
    "prdtd_bias, prdtd_weights, costs = fit.gradient_descent(np.array([[1],[2],[3],[4],[5],[6],[7],]), np.array([0,0,0,1,1,1,1]), bias, weights)\n",
    "print(f\"\\n\\nfinal bias = {prdtd_bias} \\t final weights = {prdtd_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7418040-fc98-4f50-90fd-06e8d62e85fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(type(target[0]))\n",
    "print(type(features[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52995e29-72b1-43b6-af11-73a47da58891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02668786, 0.67391261],\n",
       "       [0.6716525 , 0.62424072],\n",
       "       [0.22020754, 0.51400062]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = np.random.rand(3,2)\n",
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09e3d854-e883-400b-b7e4-3d135e6670db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97366512, 0.50971037],\n",
       "       [0.51086368, 0.535668  ],\n",
       "       [0.80235226, 0.59809803]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82e5ad00-9391-479f-ac57-e8eb70ae63bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    " 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    " 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    " 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    " 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    " 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    " 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    " 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
    " 0.0]\n",
    "\n",
    "np.exp(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7874b117-6e44-4479-bb4f-3e67bda1ede0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
